# Context-Aware Sensitive Text Redaction with VLM

Мультимодальный сервис для **поиска и удаления чувствительной информации на изображениях** (схемы, диаграммы, инфраструктурные рисунки, скриншоты).

Проект показывает **применение Vision-Language Model (VLM)** в связке с OCR, где:

* чувствительность текста определяется **визуальным контекстом**
* текст сначала **интерпретируется моделью**
* затем **локализуется на изображении и удаляется**

## Задача проекта

**Context-aware sensitive text extraction & redaction**

На вход:

* Изображение (диаграмма, схема, скриншот)

На выход:

* Изображение, где **все чувствительные участки автоматически закрашены**

Чувствительность определяется **не по ключевым словам**, а по:

* визуальному окружению
* типу диаграммы
* семантическому контексту (инфраструктура, персональные данные, внутренние сервисы)

## Архитектура решения

Пайплайн состоит из **нескольких независимых ML-этапов**.

```
┌──────────┐
│  Image   │
└────┬─────┘
     ├─────────────────────┐
     │                     │
     ▼                     ▼
┌────────────────────┐┌────────────────────┐
│ Vision-Language    ││ OCR (easyocr)      │
│ Model (VLM)        ││                    │
│                    ││ • Bounding boxes   │
│ • Анализ картинки  ││ • Координаты       │
│ • Контекст         │└────┬───────────────┘
│ • Отбор            │     │
│   чувствительного  │     │
│   текста           │     │
└────┬───────────────┘     │
     ├─────────────────────┘
     │ список фраз и координат всего текста
     ▼
┌────────────────────┐
│ Text-to-box match  │
│                    │
│ • fuzzy matching   │
└────┬───────────────┘
     │
     ▼
┌────────────────────┐
│ Image redaction    │
│                    │
│ • закрашивание     │
└──────────┬─────────┘
           ▼
     Final Image
```


## Используемые модели и инструменты

### Vision-Language Model

* `Qwen2-VL-2B-Instruct`
* Используется **только для контекстного решения**
* Не занимается локализацией

### OCR

* `easyocr` (в данном случае)
* или `PaddleOCR`

### UI

* Gradio

## Структура проекта

```
vlm-sensitive-text-redaction/
│
├── examples                # Примеры
├── notebooks               # Ноутбуки
├── scripts                 # Скрипт для запуска
├── src                     # Код приложения
│
├── requirements.txt
└── README.md
```

## Установка

### 1. Клонирование репозитория

```bash
https://github.com/mark-polokhov/vlm-sensitive-text-redaction.git
cd vlm-sensitive-text-redaction
```

### 2. Создание виртуального окружения

```bash
python -m venv venv
source venv/bin/activate
```

### 3. Установка зависимостей

```bash
pip install -r requirements.txt
```

## Запуск сервиса

```bash
python src/app.py
```

После запуска Gradio выведет ссылку:

```
Running on http://127.0.0.1:7860
```

Откройте её в браузере.


## Как пользоваться

1. Загрузите изображение
2. Нажмите **Submit**
3. Модель:
   * определит чувствительный текст
   * найдет его координаты
   * закрасит соответствующие области
4. Скачайте отредактированное изображение

## Возможные улучшения

* GroundingDINO вместо OCR-box match
* Большая модель (на моих ресурсах я ограничен парой миллиардов параметров)
* Классификация типов чувствительности
* batch-инференс

## Ограничения

* Тестирование производилось на NVIDIA RTX 4070 SUPER (12Gb)
* На этих ограничениях сервис запускается приблизительно 10 сек
* Время инференса картинки в Full-HD составляет около 5 сек
