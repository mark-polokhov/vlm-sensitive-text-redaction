{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d35f8eeb",
   "metadata": {},
   "source": [
    "# VLM + OCR Grounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8906d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"X:/Programming/Models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e973682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q pandas numpy matplotlib seaborn scikit-learn tqdm\n",
    "# %pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "# %pip install -q transformers pillow nltk evaluate accelerate kagglehub\n",
    "# %pip install -q rouge_score\n",
    "# %pip install -q jupyter ipywidgets\n",
    "# %pip install -q easyocr[ru]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3625d78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x:\\Programming\\Education\\master_2\\multimodal\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "import easyocr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4512a498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version: 2.6.0+cu124\n",
      "CUDA status: Available\n",
      "CUDA Device Count: 1\n",
      "GPU Device: NVIDIA GeForce RTX 4070 SUPER\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch Version:\", torch.__version__)\n",
    "print(\"CUDA status:\", \"Available\" if torch.cuda.is_available() else \"Not Available\")\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "print(\"GPU Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba2995f",
   "metadata": {},
   "source": [
    "### VLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "172fe0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "x:\\Programming\\Education\\master_2\\multimodal\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:2284: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a05d7a0c044f8391dac465ca0da5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865b910f",
   "metadata": {},
   "source": [
    "### OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "189ee5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = easyocr.Reader(['ru', 'en'], gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d422f10",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69887cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"image_2.jpg\"\n",
    "image = Image.open(image_path).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e81fea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Extract ONLY sensitive text from the image.\n",
    "\n",
    "Rules:\n",
    "- Output ONLY text that appears verbatim in the image\n",
    "- Sensitivity must depend on visual context\n",
    "- One item per line\n",
    "- No explanations\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": prompt}\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cccff70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache\n",
      "Kafka\n",
      "Spark\n",
      "Streaming\n",
      "Apache\n",
      "HBase\n",
      "elasticsearch\n",
      "MySQL\n"
     ]
    }
   ],
   "source": [
    "text = processor.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "output_vlm = []\n",
    "\n",
    "for item in processor.decode(outputs[0], skip_special_tokens=True).split(\"assistant\\n\")[-1].split('\\n'):\n",
    "    output_vlm.extend(item.split())\n",
    "\n",
    "output_vlm = [item for item in output_vlm if len(item) > 3]\n",
    "print('\\n'.join(output_vlm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a22b5a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[np.int32(731), np.int32(81)],\n",
       "  [np.int32(835), np.int32(81)],\n",
       "  [np.int32(835), np.int32(97)],\n",
       "  [np.int32(731), np.int32(97)]],\n",
       " 'Following Task',\n",
       " np.float64(0.9999287997170756))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ocr = reader.readtext(image_path)\n",
    "output_ocr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d705298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Following Task'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ocr[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f2a2b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Following Task',\n",
       " 'bbox': [[np.int32(731), np.int32(81)],\n",
       "  [np.int32(835), np.int32(81)],\n",
       "  [np.int32(835), np.int32(97)],\n",
       "  [np.int32(731), np.int32(97)]],\n",
       " 'score': np.float64(0.9999287997170756)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocr_items = []\n",
    "for line in output_ocr:\n",
    "    bbox = line[0]\n",
    "    text = line[1]\n",
    "    score = line[2]\n",
    "    ocr_items.append({\n",
    "        \"text\": text,\n",
    "        \"bbox\": bbox,\n",
    "        \"score\": score\n",
    "    })\n",
    "\n",
    "ocr_items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8783da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grounded = []\n",
    "\n",
    "for phrase in output_vlm:\n",
    "    for item in ocr_items:\n",
    "        if phrase.lower() in item[\"text\"].lower():\n",
    "            grounded.append({\n",
    "                \"text\": phrase,\n",
    "                \"bbox\": item[\"bbox\"]\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b78bd588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Kafka', 'MySQL', 'Spark', 'Streaming', 'elasticsearch'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([item['text'] for item in grounded])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b091ea",
   "metadata": {},
   "source": [
    "## Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1213f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_results(image_pil, results):\n",
    "    draw = ImageDraw.Draw(image_pil)\n",
    "    \n",
    "    for item in results:\n",
    "        bbox = item['bbox']\n",
    "        \n",
    "        points = []\n",
    "        for point in bbox:\n",
    "            points.extend([point[0], point[1]])\n",
    "\n",
    "        fill_color = (0, 0, 0, 64)  # RGBA с прозрачностью 64/255\n",
    "        outline_color = (0, 0, 0)\n",
    "        \n",
    "        draw.polygon(points, fill=fill_color, outline=outline_color, width=8)\n",
    "    \n",
    "    return image_pil\n",
    "\n",
    "# Рисуем bbox на изображении\n",
    "annotated_image = draw_results(image.copy(), grounded)\n",
    "\n",
    "# Сохраняем результат\n",
    "annotated_image.save('ocr_with_boxes.jpg', 'JPEG', quality=95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0c04e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
